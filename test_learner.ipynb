{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from bfair.sensors.image.clip.optimization import get_tokens_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "attributes = [\"male\", \"female\"]\n",
    "attr_cls = \"gender\"\n",
    "tokens_pipeline = [[value for value in attributes]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_sensor_call(item, attributes: List[str], attr_cls: str):\n",
    "    \"\"\"\n",
    "    Calls a ClipBasedSensor execution.\n",
    "\n",
    "    :param item: images list\n",
    "    :param List[str] attributes: attribute class values\n",
    "    :param str attr_cls: attribute class name\n",
    "    :return: labels from attributed tokens\n",
    "    \"\"\"\n",
    "    for tokens in tokens_pipeline:\n",
    "        text = clip.tokenize(tokens).to(device)\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    for i in range(0, len(item), min(BATCH_SIZE, len(item) - i)):\n",
    "        images = []\n",
    "        for photo_addrs in item[i : min(i + BATCH_SIZE, len(item))]:\n",
    "            img = Image.open(photo_addrs)\n",
    "            img_preprocess = preprocess(img)\n",
    "            img.close()\n",
    "            images.append(img_preprocess)\n",
    "\n",
    "        image_input = torch.tensor(np.stack(images)).to(device)\n",
    "        with torch.no_grad():\n",
    "            logits_per_image, _ = model(image_input, text)\n",
    "\n",
    "            batch_probs = logits_per_image.softmax(dim=-1).cpu().numpy()\n",
    "\n",
    "            attribute_probs = [[] for _ in range(len(batch_probs))]\n",
    "            for k in range(len(batch_probs)):\n",
    "                image_probs = batch_probs[k]\n",
    "                for j in range(len(attributes)):\n",
    "                    attribute_probs[k].append((attributes[j], image_probs[j]))\n",
    "\n",
    "            attributed_tokens = []\n",
    "            for h in range(i, min(i + BATCH_SIZE, len(item))):\n",
    "                attributed_tokens.append(\n",
    "                    (\n",
    "                        \"image_\" + str(i + h % BATCH_SIZE),\n",
    "                        attribute_probs[h % BATCH_SIZE],\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            results.append(attributed_tokens)\n",
    "\n",
    "    flatten_results = []\n",
    "    for batch in results:\n",
    "        for result in batch:\n",
    "            flatten_results.append(result)\n",
    "\n",
    "    return flatten_results\n",
    "\n",
    "    # for filter in self.filtering_pipeline:\n",
    "    #     attributed_tokens = filter(flatten_results)\n",
    "\n",
    "    # ### changing output\n",
    "    # labels_from_attr_tokens = []\n",
    "    # for _, labels_values_pair in attributed_tokens:\n",
    "    #     labels_from_attr_tokens.append([labels for labels, _ in labels_values_pair])\n",
    "\n",
    "    # return labels_from_attr_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('image_0', [('male', 0.93397015), ('female', 0.06602984)]),\n",
       " ('image_1', [('male', 0.3417764), ('female', 0.65822357)]),\n",
       " ('image_2', [('male', 0.4526427), ('female', 0.54735726)])]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_list = [\n",
    "    \"datasets/utkface/1_0_0_20161219140623097.jpg.chip.jpg\",\n",
    "    \"datasets/utkface/115_1_1_20170112213257263.jpg.chip.jpg\",\n",
    "    \"datasets/utkface/105_1_0_20170112213021902.jpg.chip.jpg\",\n",
    "]\n",
    "clip_sensor_call(image_list, attributes, attr_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['female'], dtype='<U6')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# get labels for each image\n",
    "image_labels = clip_sensor_call(image_list, attributes, attr_cls)\n",
    "\n",
    "# prepare data\n",
    "X = []\n",
    "y = []\n",
    "for i, (image_path, labels) in enumerate(zip(image_list[:-1], image_labels[:-1])):\n",
    "    X.append([extended_labels[1] for extended_labels in labels[1]])\n",
    "    y.append(attributes[int(image_path.split('_')[1])])\n",
    "\n",
    "# train logistic regression model\n",
    "X = np.array(X).reshape(len(X), -1)\n",
    "y = np.array(y)\n",
    "lr = LogisticRegression(random_state=0).fit(X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['female']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X = []\n",
    "for labels in image_labels[-1:]:\n",
    "    X.append([extended_labels[1] for extended_labels in labels[1]])\n",
    "list(lr.predict(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if y_i == \"\":\n",
    "            y.append(np.array([]))\n",
    "        elif isinstance(y_i, str):\n",
    "            y.append(np.array([y_i]))\n",
    "        else:\n",
    "            y.append(np.array(y_train.values[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
