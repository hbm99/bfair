# -*- coding: utf-8 -*-
"""clip_fairface_finetune.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/16Dbl5PkTUt-VZCsX5fOMcLZAOeALmqT-

Loading dataset
"""

# Commented out IPython magic to ensure Python compatibility.

import datasets as db
import pandas as pd
from random import randint, sample
from PIL import Image

MALE_VALUE = "Male"
FEMALE_VALUE = "Female"
GENDER_VALUES = [MALE_VALUE, FEMALE_VALUE]


EAST_ASIAN_VALUE = "East Asian"
INDIAN_VALUE = "Indian"
BLACK_VALUE = "Black"
WHITE_VALUE = "White"
MIDDLE_EASTERN_VALUE = "Middle Eastern"
LATINO_HISPANIC_VALUE = "Latino_Hispanic"
SOUTHEAST_ASIAN_VALUE = "Southeast Asian"
RACE_VALUES = [
    EAST_ASIAN_VALUE,
    INDIAN_VALUE,
    BLACK_VALUE,
    WHITE_VALUE,
    MIDDLE_EASTERN_VALUE,
    LATINO_HISPANIC_VALUE,
    SOUTHEAST_ASIAN_VALUE,
]

GENDER_COLUMN = "gender"
RACE_COLUMN = "race"
IMAGE_COLUMN = "image"
AGE_COLUMN = "age"
CIFAR_IMAGE_COLUMN = "img"

_GENDER_MAP = {0: MALE_VALUE, 1: FEMALE_VALUE}

_RACE_MAP = {
    0: EAST_ASIAN_VALUE,
    1: INDIAN_VALUE,
    2: BLACK_VALUE,
    3: WHITE_VALUE,
    4: MIDDLE_EASTERN_VALUE,
    5: LATINO_HISPANIC_VALUE,
    6: SOUTHEAST_ASIAN_VALUE,
}

SIZE = 20000

"""utils"""

split_seed = 0


def _get_concat_h_multi_resize(im_list, resample=Image.BICUBIC):
    min_height = min(im.height for im in im_list)
    im_list_resize = [
        im.resize(
            (int(im.width * min_height / im.height), min_height), resample=resample
        )
        for im in im_list
    ]
    total_width = sum(im.width for im in im_list_resize)
    dst = Image.new("RGB", (total_width, min_height))
    pos_x = 0
    for im in im_list_resize:
        dst.paste(im, (pos_x, 0))
        pos_x += im.width
    return dst


def _get_concat_v_multi_resize(im_list, resample=Image.BICUBIC):
    min_width = min(im.width for im in im_list)
    im_list_resize = [
        im.resize((min_width, int(im.height * min_width / im.width)), resample=resample)
        for im in im_list
    ]
    total_height = sum(im.height for im in im_list_resize)
    dst = Image.new("RGB", (min_width, total_height))
    pos_y = 0
    for im in im_list_resize:
        dst.paste(im, (0, pos_y))
        pos_y += im.height
    return dst


def _get_concat_tile_resize(im_list_2d, resample=Image.BICUBIC):
    im_list_v = [
        _get_concat_h_multi_resize(im_list_h, resample=resample)
        for im_list_h in im_list_2d
    ]
    return _get_concat_v_multi_resize(im_list_v, resample=resample)


def concat_images(image_list):
    return _get_concat_tile_resize(image_list)


def merge_attribute_values(attribute, row_i, row_j):
    attribute_value_i = row_i[attribute]
    attribute_value_j = row_j[attribute]

    if attribute_value_i == "" and attribute_value_j == "":
        return ""
    if attribute_value_i == "":
        return attribute_value_j
    if attribute_value_j == "":
        return attribute_value_i

    if (
        isinstance(attribute_value_i, list)
        and isinstance(attribute_value_j, str)
        and attribute_value_j not in attribute_value_i
    ):
        attribute_value_i.append(attribute_value_j)

    elif (
        isinstance(attribute_value_i, str)
        and isinstance(attribute_value_j, list)
        and attribute_value_i not in attribute_value_j
    ):
        attribute_value_j.append(attribute_value_i)
        attribute_value_i = attribute_value_j

    elif isinstance(attribute_value_i, list) and isinstance(attribute_value_j, list):
        attribute_value_i = list(set(attribute_value_i + attribute_value_j))

    elif (
        isinstance(attribute_value_i, str)
        and isinstance(attribute_value_j, str)
        and attribute_value_i != attribute_value_j
    ):
        attribute_value_i = [attribute_value_i, attribute_value_j]

    if isinstance(attribute_value_i, list) and len(attribute_value_i) == 1:
        attribute_value_i = attribute_value_i[0]

    return attribute_value_i


def create_mixed_dataset(data, size):
    mixed_data = pd.DataFrame(columns=data.columns)
    num_rows = len(data)
    for i in range(size):
        row_i = data.iloc[i]

        image_list = [row_i[IMAGE_COLUMN]]
        rows_to_concat = randint(0, 2)
        for _ in range(rows_to_concat):
            row_j = data.iloc[randint(0, num_rows - 1)]

            image_list.append(row_j[IMAGE_COLUMN])

            for attribute in [GENDER_COLUMN, RACE_COLUMN, AGE_COLUMN]:
                row_i[attribute] = merge_attribute_values(attribute, row_i, row_j)

        row_i[IMAGE_COLUMN] = concat_images(
            [
                sample(image_list, randint(1, len(image_list)))
                for _ in range(randint(1, len(image_list)))
            ]
        )
        mixed_data = mixed_data.append(row_i, ignore_index=True)

    return mixed_data


source_ff = db.load_dataset("HuggingFaceM4/FairFace", split="validation")

df_ff = pd.DataFrame.from_dict(source_ff)
gender = df_ff[GENDER_COLUMN].apply(lambda x: _GENDER_MAP[x])
race = df_ff[RACE_COLUMN].apply(lambda x: _RACE_MAP[x])
df_ff = pd.concat(
    [
        df_ff[IMAGE_COLUMN],
        df_ff[AGE_COLUMN],
        gender.rename(GENDER_COLUMN),
        race.rename(RACE_COLUMN),
    ],
    axis=1,
)

source_noisy_dataset = db.load_dataset("cifar100", split="test")
df_noisy = pd.DataFrame.from_dict(source_noisy_dataset)

# Remove undesired classifications (people related)
df_noisy = df_noisy[~df_noisy["fine_label"].isin([2, 11, 35, 46, 98])]
df_noisy = df_noisy[~df_noisy["coarse_label"].isin([14])]

new_df_noisy = pd.DataFrame(columns=df_ff.columns)

new_df_noisy[IMAGE_COLUMN] = df_noisy[CIFAR_IMAGE_COLUMN]


new_df_noisy = pd.concat([df_ff, new_df_noisy])

# Shuffle the rows
new_df_noisy = new_df_noisy.sample(frac=1, random_state=split_seed).reset_index(
    drop=True
)

new_df_noisy = new_df_noisy.fillna("")

mixed_data = create_mixed_dataset(new_df_noisy, SIZE)

"""Loaded dataset!

Loading CLIP model
"""

# Commented out IPython magic to ensure Python compatibility.

import torch
import clip


BATCH_SIZE = 1
EPOCH = 20

device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load("ViT-B/32", device=device, jit=False)

"""Splitting dataset"""

from torch.utils.data import Dataset, DataLoader

train_df_temp = mixed_data.sample(frac=0.8)
validation_df = mixed_data.drop(train_df_temp.index).reset_index(drop=True)
train_df = train_df_temp.reset_index(drop=True)


class NoisyFFDataset(Dataset):
    def __init__(self, dataframe, preprocess):
        self.preprocess = preprocess
        self.image = dataframe["image"].tolist()
        self.gender = dataframe["gender"].tolist()
        self.race = dataframe["race"].tolist()
        self.age = dataframe["age"].tolist()

    def __len__(self):
        return len(self.image)

    def __getitem__(self, idx):
        image = self.preprocess(self.image[idx])
        gender = self.gender[idx]
        race = self.race[idx]
        age = self.age[idx]
        return image, gender, race, age


train_dataset = NoisyFFDataset(train_df, preprocess)
validation_dataset = NoisyFFDataset(validation_df, preprocess)


def my_collate_fn(data):
    return tuple(zip(*data))


train_dataloader = DataLoader(
    train_dataset, batch_size=BATCH_SIZE, shuffle=True
)  # , collate_fn= my_collate_fn)
validation_dataloader = DataLoader(
    validation_dataset, batch_size=BATCH_SIZE, shuffle=False
)  # , collate_fn= my_collate_fn)


"""Preparing finetuning"""

attributes_queries = {}

for label in RACE_VALUES:
    attributes_queries[label] = "A photo of a person of " + label.lower() + " race."

for label in GENDER_VALUES:
    attributes_queries[label] = "A photo of a person of " + label.lower() + " gender."

# print(attributes_queries)

gender_texts = [attributes_queries[lbl] for lbl in GENDER_VALUES]
# print(gender_texts)
gender_texts = clip.tokenize(gender_texts).to(device)


race_texts = [attributes_queries[lbl] for lbl in RACE_VALUES]
# print(race_texts)
race_texts = clip.tokenize(race_texts).to(device)

from torch import nn, optim
from tqdm import tqdm


# https://github.com/openai/CLIP/issues/57
def convert_models_to_fp32(model):
    for p in model.parameters():
        p.data = p.data.float()
        p.grad.data = p.grad.data.float()


if device == "cpu":
    model.float()

loss_img = nn.CrossEntropyLoss()
loss_txt = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=1e-5)
scheduler = optim.lr_scheduler.CosineAnnealingLR(
    optimizer, len(train_dataloader) * EPOCH
)

"""Finetuning"""

best_gender_te_loss = 1e5
best_gender_ep = -1
GENDER_PLACE = 1

best_race_te_loss = 1e5
best_race_ep = -1
RACE_PLACE = 2

for epoch in range(EPOCH):
    print(
        f"running epoch {epoch}, GENDER: best test loss {best_gender_te_loss} after epoch {best_gender_ep}, RACE: best test loss {best_race_te_loss} after epoch {best_race_ep}"
    )
    step = 0
    gender_tr_loss = 0
    race_tr_loss = 0
    model.train()
    pbar = tqdm(train_dataloader, leave=False)
    for batch in pbar:
        # print(batch)
        step += 1
        optimizer.zero_grad()

        images = batch[0]

        images = images.to(device)

        gender_logits_per_image, _ = model(images, gender_texts)
        race_logits_per_image, _ = model(images, race_texts)

        gender_ground_truth = torch.zeros((BATCH_SIZE, len(GENDER_VALUES))).to(device)
        race_ground_truth = torch.zeros((BATCH_SIZE, len(RACE_VALUES))).to(device)

        for i in range(BATCH_SIZE):
            # gender
            gender_truth_idxs = []
            for item in batch[GENDER_PLACE]:
                if isinstance(item, tuple):
                    item = item[0]
                if item in GENDER_VALUES:
                    gender_truth_idxs.append(GENDER_VALUES.index(item))
            for gender_truth_idx in gender_truth_idxs:
                gender_ground_truth[i, gender_truth_idx] = 1

            # race
            race_truth_idxs = []
            for item in batch[RACE_PLACE]:
                if isinstance(item, tuple):
                    item = item[0]
                if item in RACE_VALUES:
                    race_truth_idxs.append(RACE_VALUES.index(item))
            for race_truth_idx in race_truth_idxs:
                race_ground_truth[i, race_truth_idx] = 1

        gender_total_loss = loss_img(gender_logits_per_image, gender_ground_truth)
        gender_total_loss.backward()
        gender_tr_loss += gender_total_loss.item()

        race_total_loss = loss_img(race_logits_per_image, race_ground_truth)
        race_total_loss.backward()
        race_tr_loss += race_total_loss.item()

        if device == "cpu":
            optimizer.step()
            scheduler.step()
        else:
            convert_models_to_fp32(model)
            optimizer.step()
            scheduler.step()
            clip.model.convert_weights(model)

        pbar.set_description(
            f"GENDER: train batchCE: {gender_total_loss.item()}", refresh=True
        )
        pbar.set_description(
            f"RACE: train batchCE: {race_total_loss.item()}", refresh=True
        )

    gender_tr_loss /= step
    race_tr_loss /= step

    step = 0

    gender_te_loss = 0
    race_te_loss = 0
    with torch.no_grad():
        model.eval()
        val_pbar = tqdm(validation_dataloader, leave=False)
        for batch in val_pbar:
            step += 1
            images = batch[0]

            images = images.to(device)

            gender_logits_per_image, gender_logits_per_text = model(
                images, gender_texts
            )
            gender_ground_truth = torch.zeros((BATCH_SIZE, len(GENDER_VALUES))).to(
                device
            )

            race_logits_per_image, race_logits_per_text = model(images, race_texts)
            race_ground_truth = torch.zeros((BATCH_SIZE, len(RACE_VALUES))).to(device)

            for i in range(BATCH_SIZE):
                # gender
                gender_truth_idxs = []
                for item in batch[GENDER_PLACE]:
                    if isinstance(item, tuple):
                        item = item[0]
                    if item in GENDER_VALUES:
                        gender_truth_idxs.append(GENDER_VALUES.index(item))
                for gender_truth_idx in gender_truth_idxs:
                    gender_ground_truth[i, gender_truth_idx] = 1

                # race
                race_truth_idxs = []
                for item in batch[RACE_PLACE]:
                    if isinstance(item, tuple):
                        item = item[0]
                    if item in RACE_VALUES:
                        race_truth_idxs.append(RACE_VALUES.index(item))
                for race_truth_idx in race_truth_idxs:
                    race_ground_truth[i, race_truth_idx] = 1

            gender_total_loss = loss_img(gender_logits_per_image, gender_ground_truth)
            gender_te_loss += gender_total_loss.item()

            race_total_loss = loss_img(race_logits_per_image, race_ground_truth)
            race_te_loss += race_total_loss.item()

            val_pbar.set_description(
                f"GENDER: test batchCE: {gender_total_loss.item()}", refresh=True
            )
            val_pbar.set_description(
                f"RACE: test batchCE: {race_total_loss.item()}", refresh=True
            )

        gender_te_loss /= step
        race_te_loss /= step

    if gender_te_loss <= best_gender_te_loss and race_te_loss <= best_race_te_loss:
        best_gender_te_loss = gender_te_loss
        best_race_te_loss = race_te_loss

        best_gender_ep = epoch
        best_race_ep = epoch

        best_model = model
    print(
        f"epoch {epoch}, gender_tr_loss {gender_tr_loss}, gender_te_loss {gender_te_loss}, race_tr_loss {race_tr_loss}, race_te_loss {race_te_loss}"
    )

torch.save(best_model.cpu().state_dict(), "best_model.pt")
torch.save(model.state_dict(), "last_model.pt")
